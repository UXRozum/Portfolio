[{"authors":null,"categories":null,"content":"I\u0026rsquo;m Rozum Sergey, an UX researcher passionate about understanding people and their experiences.\nIn my journey, I\u0026rsquo;ve explored diverse research fields, including marketing, politics, and user experience. I enjoy crafting creative research approaches that make our work more effective. I\u0026rsquo;m also a bit of a data visualization enthusiast—I love turning data into meaningful stories.\nThanks for stopping by and getting to know a bit about me. Feel free to explore my work and reach out if you have any questions.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/en/author/rozum-sergey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/rozum-sergey/","section":"authors","summary":"I\u0026rsquo;m Rozum Sergey, an UX researcher passionate about understanding people and their experiences.\nIn my journey, I\u0026rsquo;ve explored diverse research fields, including marketing, politics, and user experience. I enjoy crafting creative research approaches that make our work more effective.","tags":null,"title":"Rozum Sergey","type":"authors"},{"authors":null,"categories":null,"content":"There can never be too many articles dedicated to fundamental topics. That’s why I also decided to dig into the well-worn and widely known subject. Today we’ll talk about the canonical definition and description of a usability issue.\nThis article deliberately avoids case studies and examples because, right now, it’s especially important to focus on the fundamental principles of usability testing without delving into isolated examples.\nWhy do I think this is important? Usability testing is a research method with surprisingly few ambiguities. It is very limited and straightforward, but it is precisely in these limitations that its strength lies. The same applies to its results; we can very clearly define what a usability issue is and how to describe it.\nHowever, we often feel that we can use this method to cover more than it’s intended for. Such blurring can strip the method of its strengths and reduce trust in our research.\nFeatures of Usability Testing as a Method In usability testing, we have a clearly defined object of study — the interface. Our goals and objectives are also unambiguous — we are looking for usability issues encountered by users and measuring the usability of the product being tested.\nBecause the object of usability testing is the interface, we can make strong conclusions with a small sample size. We don’t need to worry much about the validity of results, as we’re not studying opinions but the interface itself. We literally see the interface features that lead to problems during the test and understand that they need to be addressed in some way.\nBy limiting the scope of the research, we gain a unique advantage in qualitative studies. Our conclusions are far less speculative; we have solid, rigorous arguments. This narrow focus allows us to make conclusions with a sample size that would be microscopic for any other study.\nMost criticism of usability testing results stems from a lack of understanding of these method features. “Can we really trust these participants? Sure, they say it’s inconvenient, but it’s just two people,” or, “How can we be sure other users will encounter the same issues? Maybe your participants are just not smart.” All of this is meaningless if we understand that usability testing evaluates not users, not even their behavior, but the interface. Mistakes made by participants during the test help us identify interface elements that may cause problems in real-world use and understand why they occur. In usability testing, the participant acts as a tool, helping us evaluate the interface.\nHowever, if we shift the research focus from the interface to the user, problems begin to arise. As a method, usability testing has simple sampling requirements but very limited capacity for studying user opinions and behavior. If we try to study users using this method, our sample size increases, we need to create a 4–8-window structure, and our scenario becomes more complex with added variability. This complicates the analysis, increases our report size, and effectively turns the study into in-depth interviews with elements of usability testing, rather than usability testing itself. This significantly impacts the timelines and costs of the research.\nAlternatively, we might not modify the test methodology but attempt to work with participants and results as if conducting in-depth interviews. This approach likely fails to reach data saturation, resulting in shaky foundations for conclusions that are over-extrapolated. Ultimately, this diminishes the value of the research and undermines its validity in the eyes of stakeholders.\nTherefore, to fully leverage the strengths of usability testing — speed, simplicity, low sample requirements, and clear, unambiguous results — we must consciously keep our research focus under control.\nResults in Usability Testing In usability testing, we must always distinguish between primary and secondary results. The primary result of usability testing is always a set of usability issues. It may also include insights into the overall usability of the product but nothing more. User expectations, feature requests, product opinions, and usage patterns are also incredibly important, but they are secondary in this particular study. To explore them, we need to use a more appropriate research method. Usability testing will not provide sufficiently strong results in these areas — at most, it will generate hypotheses and ideas for subsequent, more methodologically valid studies.\nThis brings us to the idea of the ideal usability issue description.\nTo describe the ideal usability issue, we turn to the classic definition of usability:\n Extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use.\n From this definition, we can derive the definition of a usability issue:\n A usability issue is a feature of the interface that hinders the effective and efficient completion of user tasks and reduces user satisfaction with the system.\n This definition is important because it also allows us to outline what a usability issue is not:\nA usability issue cannot be the lack of desire to perform an action in the product. It cannot manifest as a user not wanting to do something. We deliberately narrow our focus in usability testing to exclude questions of willingness to use a product or its features. Studying feature demand is more complex than studying usability and requires a different research method.\nA usability issue cannot be a bug because a bug is inherently incorrect system behavior. If we try to study bugs or technical system properties (e.g., data loading speed, animation performance) through usability testing, we risk drawing conclusions from an inherently flawed sample. Proper technical testing requires a dedicated device setup and specialized testing methods. That’s why QA testing is often a separate function in a company, operating on a different principle than usability testing.\nA usability issue cannot be a lack of functionality. In usability testing, we study the ease of use of existing product features. We may receive comments about missing features, but these should always serve as a basis for further research, not as standalone results.\nA usability issue is not the absence of a solution because a problem is a problem, not the absence of a solution. This distinction is subtle. A usability issue is a property of the interface, an element that hinders product use. If we frame a problem as “Feature X is missing,” we miss the opportunity to address the usability issue in other ways. It’s like going to a doctor and saying, “I lack painkillers” instead of “I have a headache.” While technically possible, this approach limits us to one solution that may not be the best.\nThe Ideal Usability Issue Based on all the above, we can define the ideal usability issue description:\nA usability issue should include:\n  A description of the interface property that causes the problem. We need to describe how the problematic element works, as it is the source of the issue.\n  An explanation of why this system behavior is problematic. This allows us to describe the problem\u0026rsquo;s mechanism and argue why we consider it a usability issue. It’s especially important to outline the participant’s actions with the problematic element that lead to difficulties.\n  A description of the problem’s impact on usability. By definition, usability issues reduce key usability metrics, and it’s essential to demonstrate this negative impact.\n  We can further illustrate the usability issue with participant quotes and add recommendations for resolution.\nAs we see, participant opinions are not the essence of a usability issue but its illustration. This is particularly important because the object of our study is the interface, not the user.\nBy strictly adhering to the constraints of usability testing, conducting and processing results will take significantly less time and resources compared to in-depth interviews. This is why I believe usability testing should be as simple as a hammer. Its strength lies in these limitations.\nIn this article, I’ve tried to thoroughly describe my understanding of usability testing and the description of its results. As with everything I write, this represents my personal professional opinion rather than a textbook or objective truth. In any case, feel free to write to me and share feedback — I always enjoy and appreciate reading it.\n","date":1715990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715990400,"objectID":"cba1c17149ab432077f2cb3722fde46d","permalink":"/en/post/ideal-usability-problem/","publishdate":"2024-05-18T00:00:00Z","relpermalink":"/en/post/ideal-usability-problem/","section":"post","summary":"Analyzing how to properly describe usability issues and what they are not","tags":["Usability Testing","Usability Issues"],"title":"The Ideal Usability Issue","type":"post"},{"authors":null,"categories":null,"content":"TL;DR: Our approach to innovations is incorrect, leading to futile research. Let\u0026rsquo;s adopt a more suitable language for describing and researching innovations, and everything will fall into place.\nDisruptive Innovations When we talk about innovation, something grand immediately comes to mind. In popular culture, a startup founder is always someone aiming to change the world. In product development culture, the concept of \u0026ldquo;disruptive innovation\u0026rdquo; — an innovation that fundamentally changes the market — remains alive and well. It is contrasted with \u0026ldquo;sustaining innovation,\u0026rdquo; which merely improves existing products.\nThe term \u0026ldquo;Disruptive Innovation\u0026rdquo; was popularized by Clayton Christensen in his article \u0026ldquo;Disruptive Technologies: Catching the Wave.\u0026quot; The article discusses the evolution of product characteristics and how companies respond to this evolution. He argues that the development of a specific technical attribute (e.g., sound quality in radios) often reaches the limits of current technology. Technologies that prioritize seemingly less important attributes at the expense of the primary ones become \u0026ldquo;disruptive,\u0026rdquo; creating new markets and applications. For instance, Sony sacrificed sound quality to create portable radios. Thus, the article concludes that companies should pay less attention to user preferences and more to technologies that seem misaligned with those needs. New technologies create new markets and new users.\nThis idea makes sense when talking about technology. However, product owners often transfer this understanding from the technological realm to the semantic realm. We assume that the right product radically alters established habits. Examples like Amazon, Netflix, and Twitter are often cited as having upended many familiar practices. If people can\u0026rsquo;t accurately predict the technical features they need, then their habits and opinions can also be ignored — they’ll adapt.\nIn reality, it’s the opposite. A product or technology is never the source of behavioral change. The better a product integrates into our existing consumption patterns, the more it adapts to and fits our needs, the more likely it is to create a \u0026ldquo;market revolution.\u0026rdquo; Misunderstanding this leads us to extend the principle of disruptive innovation from product attributes into the very life of potential users.\nA popular (though dubious) quote attributed to Henry Ford illustrates this point: \u0026ldquo;If I had asked people what they wanted, they would have said, \u0026lsquo;a faster horse.'\u0026rdquo; What’s often overlooked is that even if Ford didn’t literally create a faster horse, he still relied on existing habits around machinery. For example, early tractors weren’t just vehicles; they also powered various farming operations. Cars seamlessly integrated into everyday life, and only later did society evolve new patterns that spurred further automotive development.\n\r\rHere we see how farmers, accustomed to horses, modified Ford\u0026rsquo;s tractor to work with their familiar horse-drawn carts and plows\r\r\rIf we examine our own experience with digital technology, we’ll find that the cause-and-effect relationship is cyclical, not linear. It’s not “product → behavior” but “behavior → product → behavior.” Instagram didn’t teach people to post or edit photos online. It simplified an action people were already doing. Then users adapted the product to their existing habits, and only after that did the practice of photo posting evolve.\nHuman habits and communication patterns are the biggest sources of innovation, not the other way around. For innovation to succeed, it shouldn’t change lives; at first, it must integrate into them.\nThe Tragedy of the Milkshake A simple example from my life: in my bedroom, there’s a heater. It’s internet-connected and comes with a remote control. Obviously, the remote is more convenient than a smartphone app — you just reach over and press a button. In contrast, using the phone requires opening the app, selecting the heater, and then turning it on.\nBut I’ve noticed a pattern: when I’m in bed before sleep, I turn on the heater using my phone instead of the more straightforward remote lying right next to me. Why? The answer is simple: my phone is always in my hand. The smartphone is an extension of myself, making it easier to click a few buttons than to switch contexts to a separate device. Using the phone integrates better into my daily life than the seemingly more convenient remote.\n\r\rThe remote seems faster and easier, but in reality, the phone is always at hand, while the remote is not\r\r\rIgnoring the context of product usage can lead to seemingly logical, beneficial decisions that ultimately fail to gain popularity. Users would need to adapt too much to use such solutions. New products and features should act like a missing puzzle piece, seamlessly fitting into the user\u0026rsquo;s environment. When we aim to make product development revolutionary, we end up forcing users into something unnatural.\nThis is an obvious thought and is repeatedly emphasized in the Jobs To Be Done (JTBD) framework. One of its popularizers, Christensen, also coined the term “Disruptive Innovations.” JTBD is well-known and widely used, advocating for studying the context of product usage. Yet, based on my experience with its practical application, I believe we often misunderstand its essence — and perhaps even Christensen himself did.\nMy attitude toward JTBD has shifted over time. At first, I saw it as a trendy buzzword for people unfamiliar with marketing. Then, it grew in my eyes to a product creation framework, a useful metaphor for IT professionals. It establishes its terminology, better describing the structure of human needs for \u0026ldquo;techies.\u0026rdquo;\nBut now I see that JTBD contains a profound intuition about the essence of innovation. It seems Christensen struggled to convey this clearly, and after being distilled through courses and simplifications, JTBD has lost much of its essence for many.\nThe overused \u0026ldquo;milkshake case\u0026rdquo; is particularly illustrative (though later debunked in Jim Kalbach’s Jobs to Be Done Playbook). This case is often cited as an example of innovation that dramatically boosted milkshake sales. I won’t retell it — you can read it here. It seems simple: talk to users, understand what job they’re \u0026ldquo;hiring\u0026rdquo; the product for (sometimes even asking directly), and they’ll tell you what features to implement.\nIn reality, it’s more complex. The habit of eating before work has always existed and will continue, regardless of milkshakes. Milkshakes simplify the act of eating on the way to work with minimal effort. After studying this process, creators realized that thickening the milkshake would make it better suited for this context. Despite its depth, this case is often reduced to, \u0026ldquo;Just talk to the user, and they’ll tell you what they need.\u0026rdquo; I find this tragic.\nThe terms Disruptive Technologies and the milkshake example from JTBD are often interpreted in opposing ways. We’re told not to focus on user needs, yet simultaneously to talk to users so they can tell us what they require.\nResolving the Contradiction This contradiction arises only when JTBD is misunderstood. Christensen clearly differentiates between \u0026ldquo;function\u0026rdquo; and \u0026ldquo;job.\u0026rdquo; This may seem like semantics, but the distinction is critical for understanding product usage. The term \u0026ldquo;job\u0026rdquo; reflects the fact that users adapt products to themselves, not the other way around.\nPeople don’t engage in forums because forums facilitate interaction. Forums aren’t the source of interaction. People learned to use forums for interaction. While this might sound like the same thing, it’s diametrically opposite from a product development perspective. Instead of inventing a new function to keep users engaged, we must allow users to adapt the product to more of their existing habits. Only then will they use it more frequently.\nWords are incredibly important. How we talk about something can reveal a lot. For example, a good marker of incorrect attitudes toward research is the phrase \u0026ldquo;Let the users say what’s better.\u0026rdquo; Just like with a cocktail, we expect that if we ask the question in the right way, people will say \u0026ldquo;It should be thicker,\u0026rdquo; after which we immediately take this to development. And there it is — innovation, growth in metrics, new markets. But in reality, such a direct process doesn\u0026rsquo;t exist.\nOur business task goes through a long and painful process of operationalization. The same happens in reverse — observations turn into insights, insights into conclusions, conclusions into decisions. We do not ask users what we should do and how. By studying people, we identify actions in which using our product can simplify their execution with minimal cost to the user, and each person may have different costs. This is why we conduct research. Reflecting on and understanding why I use my phone instead of a remote control only became possible because, as a researcher, I can look at my behavior from different perspectives. This is my primary skill and bread. And it would be strange to expect such reflexivity from people who don\u0026rsquo;t engage in studying others\u0026rsquo; behavior in their work. By expecting users to reflect on their use of digital products, we are shifting the work of researchers and product owners onto them.\nIn the everyday understanding of JTBD, this very important nuance is lost. Perhaps because the dichotomy \u0026ldquo;function/work\u0026rdquo; is not the best language to describe what’s happening. We think we are working directly with opinions, whereas we need to study behavior. This is where sociology comes to our aid. To come up with a quality innovation, we must study actions, many of which users do not reflect upon. This requires a new perspective for product research.\nResearch Perspective and Descriptive Language Bruno Latour and his “Reassembling the Social” significantly altered many people’s understanding of how we should study and describe society and processes within it. Latour talks a lot about how we should map the social landscape. In Actor-Network Theory, the subjects of society become not only people but also everything around them.\n Is there a big or small difference between a woman driving, slowing down near a school when seeing a yellow \u0026ldquo;no more than 30 mph\u0026rdquo; sign, and a driver slowing down to avoid damaging his car\u0026rsquo;s suspension, which is at risk of hitting a \u0026ldquo;speed bump for speedsters\u0026rdquo;? The difference is big because the first is motivated by morality, symbols, road signs, the yellow color, and the second by the same list, with the addition of a thoughtfully placed concrete slab. But the difference is small because both are obeying something: the first is obeying infrequently manifested altruism: if she hadn’t slowed down, her conscience would be tormented by the moral law; the second is obeying considerable self-interest: if he hadn’t slowed down, his car\u0026rsquo;s suspension would have been damaged by the concrete slab. Should we say here that only the first attitude is social, moral, or symbolic, while the second is objective and material? No. But if we say both attitudes are social, how can we justify the difference between moral behavior and suspension springs? Relations may not be social throughout the whole process, but they are certainly brought together or connected by the engineers who designed the road. One cannot call themselves a sociologist and follow only certain relationships — moral, legal, and symbolic — stopping as soon as some physical relation is involved. That would make any research impossible.\n– B. Latour, “Reassembling the Social. An Introduction to Actor-Network Theory”\n We must not lose sight of anything. Every little thing, like a trash bin or a door, can influence our practices and behavior. As researchers in digital products, we must study how people produce their practices, and in our perspective, digital products and devices must be treated as subjects of interaction just like people.\nAt the beginning of my article, I mentioned that it’s easier for me to turn on the heater in my bedroom with my phone because I’m usually already holding it and don’t want to switch my attention to the remote control lying nearby. It seems like a logical conclusion — the closer the objects to each other, the easier it is for us to interact with them. But that\u0026rsquo;s not true!\nConsider the reverse example. When I need to quickly switch between two browser tabs, I separate them into two different windows. It seems like switching windows is an extra operation, but tabs were invented to avoid wasting time switching between windows. However, I already switch between windows constantly, and the \u0026ldquo;alt+tab\u0026rdquo; combination is ingrained in my subconscious, so it\u0026rsquo;s faster for me to switch between two windows than between two tabs in the same window. Describing this pattern would be impossible without understanding the system\u0026rsquo;s properties and how switching happens. Hotkeys and how they work are also a part of the interaction. I use this feature not because it is more convenient in a vacuum, but because it better fits into my usual pattern of computer use in this specific operation.\nTo properly use this perspective, we also need a slightly different approach to research. Among product researchers, it is largely understood, so much so that it seems obvious to us. But I think it’s important to say it again. The main method in our work is not interviews, but observation. Even when we talk to someone, we observe their responses and make conclusions based on our observations. The same applies to JTBD. The milkshake case is possible not because we asked the clients correctly, and the clients answered correctly. It became possible only due to a clear and, most importantly, detailed description of the pre-work snack practice. This description wouldn’t have been possible without observation, and in the description of this case, it’s evident — first, the specialists stood in the parking lot and observed who bought what and in what situation. Then they talked to people to understand why it happened that way. The best JTBD research, the best research for innovation — is observation.\nFuture Perspectives Ideally, it would make sense to turn to the more academic environment here. Because ethnomethodology of digital product use would be incredibly useful. But unfortunately, there aren’t many works on this subject. Digital ethnography is now more focused on analyzing digital artifacts of human activity, not on how they produce that activity. Therefore, we will likely have to reflect on the experience of using digital products ourselves through the lens of Actor-Network Theory. This is what I encourage everyone to do.\nLet’s strive to map our users' practices as thoroughly as Latour advises. The process of using a digital product is very complex and multifaceted, yet we are currently oversimplifying it. And the chain \u0026ldquo;find an image on Google, copy it, clean it from watermarks, crop it in Photoshop, then insert it into a PowerPoint presentation\u0026rdquo; turns into \u0026ldquo;add an image to the presentation.\u0026rdquo; It is in the detailed study and description of the practices of using digital devices and products that lies the key to creating successful innovations. Detailed descriptions of practices are key to successful exploratory research.\nAs long as we continue to perceive our research simplistically and innovation as a revolution, we will be doomed to develop our products literally by trial and error. This isn’t necessarily a bad thing, especially given that modern development frameworks now aim to reduce the cost of failure. But if we are to engage in research, let’s do it as effectively as possible. The more observational elements there are in our research work, the more useful our insights will be. The more the approach to innovation is focused not on producing a revolution but on embedding our product into familiar patterns, the more useful our products will become.\nIf you liked the article, feel free to share it, leave your comments and feedback at mail@uxrozum.com, or on telegram.\n","date":1704412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704412800,"objectID":"74fd141604cd24868429b5a14becef6e","permalink":"/en/post/innovatons/","publishdate":"2024-01-05T00:00:00Z","relpermalink":"/en/post/innovatons/","section":"post","summary":"Why Your Innovations Fail to Stick and How JTBD Relates","tags":["Jobs To Be Done","Innovations","Digital Ethnography"],"title":"Innovations — Searching for a New Descriptive Language","type":"post"},{"authors":null,"categories":null,"content":"Card sorting is an excellent method for organizing the information architecture of a website or application. It\u0026rsquo;s easy to collect data, but processing it can be challenging. Manually handling the dataset is quite tedious.\nIf you used an online service to collect the data, you might try their built-in processing tools, but they don\u0026rsquo;t always yield clear results. We can spend hours staring at a correlation table without understanding what to do with it. Dendrograms are better, but not all platforms allow you to build them, and they often look confusing.\nToday, I’d like to share a simple and straightforward way to process card sorting results so that you don\u0026rsquo;t get lost and can present them clearly to your clients.\nA Bit About Card Sorting Card sorting is a qualitative-quantitative method to find the most optimal grouping of entities. You can use it to build user-friendly app navigation or categorize products into understandable groups.\nParticipants are given cards to divide into groups in any logical way they prefer.\nIf you have predefined groups, it\u0026rsquo;s called closed sorting. If participants can create as many groups as they want and name them, it\u0026rsquo;s called open card sorting.\n\r\rWhat card sorting looks like\r\r\rIt\u0026rsquo;s good practice to discuss with participants why they grouped cards the way they did. In-person research works better for this than remote.\nThere\u0026rsquo;s also reverse card sorting, or tree testing, which can test an already structured system.\nIn this case, we\u0026rsquo;ll talk about open card sorting.\nAbout the Data We\u0026rsquo;ll Use When I thought about this article, I couldn\u0026rsquo;t find open test datasets for card sorting. Generating data myself wasn’t appealing, so I decided to collect my own dataset, available to everyone.\n\r\rThe post inviting participants to the survey\r\r\rThe task was to group 20 different animals. I deliberately designed the cards to avoid obvious grouping options:\n\r\rThe cards to be sorted\r\r\rI received 107 responses. Thanks to everyone who participated! I cleaned the collected data and ended up with this table:\n\r\rRaw data, available for download\r\r\rAlmost all variables were created automatically; I added a \u0026ldquo;Group_tag\u0026rdquo; column to tag groups based on my classification criteria.\nThe distribution was as follows:\n\r\rMixed grouping refers to cases where no clear basis for grouping could be identified. Biological grouping follows basic biology lessons (e.g., reptiles, birds, mammals), with fictional animals often placed in a separate category.\nAnalyzing Results To process the results, we need the RStudio program. It\u0026rsquo;s free and can be downloaded here. RStudio works on both Windows and Mac.\n\r\rThe RStudio interface\r\r\rDownload the ready-to-use script here and open it. I’ve detailed each step.\nPreparing for Work First, install and load the necessary libraries:\n1 2 3 4 5 6 7 8 9 10  #Step 0: Install libraries (run only once after installing R) install.packages(c(\u0026#39;openxlsx\u0026#39;,\u0026#39;igraph\u0026#39;, \u0026#39;factoextra\u0026#39;, \u0026#34;ggwordcloud\u0026#34;, \u0026#39;rstudioapi\u0026#39;)) # Step 1: Load libraries library(igraph) library(openxlsx) library(factoextra) library(ggwordcloud) library(rstudioapi)   To run the code, highlight it and press ctrl+enter.\nAfter installation, you\u0026rsquo;ll see this message:\n\r\rFull code available here\r\r\rWhen loading packages with the library command, you’ll get warnings — no need to worry about them:\n\r\rFull code available here\r\r\rNow we’re ready to work with the data.\nLoading Data into RStudio First, load the Card.xlsx file from here. Place it in a separate folder (preferably with a Latin-based name).\nTo upload the data from Card.xlsx, use the following commands:\n1 2 3 4 5 6 7 8 9 10 11 12  #Step 2: Select a folder setwd( rstudioapi::selectDirectory()) #This should be a separate folder on your computer #with a Latin name #The folder should contain a data file - Card.xlsx #Step 2.5: Load data Raw \u0026lt;- read.xlsx(\u0026#39;Card.xlsx\u0026#39;) #The file must have at least three columns #Card - card names #Group_id - group ID (unique for EACH group) #Group_name - group names provided by respondents   RStudio will prompt you to select the folder containing Card.xlsx. Analysis results will also be saved in this folder.\n\r\rNow we have the Raw dataset to work with\r\r\rCreating a Adjacency Table A adjacency table shows how often each pair of cards appeared in the same group. Use the following commands to create and save it:\n1 2 3 4 5 6 7  #Step 3: Create an adjacency table Adj \u0026lt;- crossprod(table(Raw$Group_id, Raw$Card)) diag(Adj) = 0 #Step 4: Save the adjacency table write.xlsx(as.data.frame(Adj), \u0026#39;Adjacency.xlsx\u0026#39;, overwrite = T, col.names = T, row.names=T)   The resulting file is called Adjacency.xlsx. Let’s take a look inside:\n\r\rThe adjacency table\r\r\rFor instance, the cards \u0026ldquo;Viper\u0026rdquo; and \u0026ldquo;Basilisk\u0026rdquo; were grouped together 9 times out of 107. Let’s tidy up the table and add conditional formatting in Excel:\n\r\rThe same table, now with conditional formatting\r\r\rNow it’s clearer. For example, we can see that the Basilisk often appears with the Dragon (92 times) and the Kraken (91 times).\nBut it’s still unclear how to group and interpret these results.\nClustering (Dendrogram) Run the next step in the script:\n1 2 3 4 5 6 7  #Edge betweenness algorithm -  #https://en.wikipedia.org/wiki/Girvan%E2%80%93Newman_algorithm Net \u0026lt;- graph_from_adjacency_matrix(Adj, mode=\u0026#39;undirected\u0026#39;) #Build network structure based on the adjacency table Clust \u0026lt;- as.dendrogram(cluster_edge_betweenness(Net)) #Build clusters fviz_dend(Clust, k=6 #Number of groups to obtain ,horiz=T) #Display the plot   The result is the following graph:\n\r\rDendrogram with 6 group divisions\r\r\rLet’s break it down. The edge-betweenness algorithm removes the least-connected cards sequentially, allowing stable groups to emerge. The dendrogram shows how to best divide the cards into a set number of groups. Here we used 6, but the number can vary from 1 to 20 (since we have 20 cards).\n\r\rHow the dendrogram changes with varying group numbers\r\r\rWe can now draw conclusions about optimal groupings, but the dendrogram is still not very intuitive, especially for those unfamiliar with them.\nBuilding a Network Graph (Visual Analysis of Results) While dendrograms are common in card sorting analysis, network graphs are rare.\nA great article about analyzing card-sorting data using graph visualization is Analyzing Card-Sorting Data Using Graph VisualizationJUS (uxpajournal.org). However, this type of visualization is seldom used elsewhere.\nA network graph visualizes connections between entities, where each entity is a node and each connection is an edge.\n\r\rExample of a network graph for the show Mad Men, where each node represents a character and each edge represents a sexual connection between them\r\r\rLet’s build a similar graph for our cards, where nodes represent cards and edges represent their grouping.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  #Step 6: Build a network graph plot.new() plot.igraph( graph_from_adjacency_matrix(ifelse(Adj-10 \u0026lt; 0, 0, Adj-10) #This is the number of connections to filter out,  #the higher the number, the stronger the connections #if the graph looks cluttered, increase this number #focus on the number of respondents , mode=\u0026#39;undirected\u0026#39;), vertex.label.color= \u0026#34;black\u0026#34;, vertex.color= \u0026#34;gray\u0026#34;, vertex.size= 20, vertex.frame.color=\u0026#39;gray\u0026#39;,asp = 0.7, layout = layout.kamada.kawai, vertex.label.cex = 0.9, width=1, height=1)   The result is as follows:\n\r\rNetwork graph. Beautiful, isn’t it?\r\r\rThe lines are hard to distinguish — there are too many connections. Let’s address this by iteratively reducing connections based on adjacency table values.\n1 2 3  Adj2 \u0026lt;- Adj-max(Adj)*0,2 # 80% of the strongest connections Adj2 \u0026lt;- Adj-max(Adj)*0,4 # 60% of the strongest connections Adj2 \u0026lt;- Adj-max(Adj)*0,8 # 20% of the strongest connections   For example, here’s a graph with 60% of the connections removed:\n\r\rMuch clearer now\r\r\rWe now see a mixed grouping: two large groups by habitat (aquatic and terrestrial), insects separately, and mythical creatures in their own group.\nHere’s a dynamic view:\n\r\rHow the network graph changes with connection removal\r\r\rWord Clouds We’ve dealt with the groups, but one thing remains: how to name them. Since we conducted an open card sort, the respondents themselves named the groups. We still have this data, and we’ll use it to create a word cloud for each group.\nFor example, we have the group “Cockroach-Spider-Dragonfly.” Let’s try to name it somehow.\n1 2 3  Adj2 \u0026lt;- Adj-max(Adj)*0,2 # 80% of the strongest connections Adj2 \u0026lt;- Adj-max(Adj)*0,4 # 60% of the strongest connections Adj2 \u0026lt;- Adj-max(Adj)*0,8 # 20% of the strongest connections   We end up with the following word cloud:\n\r\rWord cloud for the group ‘Cockroach-Spider-Dragonfly’\r\r\rAnd just like that, we’ve processed our results without much trouble.\nQuick Option If you don’t want to go through all the steps, there’s a quick option:\n Download two files: Cards.xlsx and CardSort.R. Place them in the same folder. Open the script. Highlight all lines from 86 (#Start) to 1139 (#End). Press Ctrl+Enter.  The script will do everything for you: generate and save a match table, create and save dendrograms (breaking down into groups from 1 to the number of cards), build 20 network graphs (from 100% connections to 5% connections), and generate word clouds for each card.\nI’ve recorded a video demonstrating how to work with the quick option for clarity:\n  In Conclusion Thank you for reading to the end; I hope you found it interesting. This script can be used to analyze a card sort you’ve conducted yourself—just insert your data into the Card.xlsx file.\nThe script and data can be downloaded on my GitHub.\nIf you liked the article, don’t hesitate to share it, and feel free to send your comments and feedback to mail@uxrozum.com or on Telegram.\n","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"07a3e7492dffd7e58c328f82e0a80bfe","permalink":"/en/post/cardsorting/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/en/post/cardsorting/","section":"post","summary":"A bit of theory, lots of practice, data processing tools","tags":["R","open card sorting","Automation"],"title":"Open Card Sorting — Quick Processing with R","type":"post"},{"authors":null,"categories":null,"content":"CardSort is an R script designed for processing card sorting data, generating visualizations, and analytical reports. This tool simplifies analysis, allowing UX researchers and designers to quickly and accurately identify user grouping logic.\nWhy do you need this script?  Time-saving. Automatic processing of data from card sorting tools like OptimalSort reduces the need for manual analysis. Informative visualization. The script generates dendrograms, similarity matrices, and other useful charts that help understand user preferences. In-depth analysis. Identifying grouping patterns and calculating consistency metrics.  How does CardSort work?  Data loading. The script accepts card sorting data in CSV format. Data processing. Based on user groupings, similarity matrices are created, and metrics are calculated. Report generation. The following are generated:  Similarity matrix showing the frequency of co-grouping elements. Dendrogram visualizing how users grouped elements. Network graph of relationships between cards. Word clouds for group names.    How to use the script? A detailed guide on using the script is provided in this article. An example of the script in action is in this video:   Download You can download this script and test data from my GitHub.\n","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"42413a200a8fd7500df3646c3b238747","permalink":"/en/project/cardsort/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/en/project/cardsort/","section":"project","summary":"Script for quick processing of open card sorting results in R","tags":["R","Open card sorting","Automation"],"title":"Script for open card sorting","type":"project"},{"authors":null,"categories":null,"content":"Guerilla testing is a controversial topic. Many researchers dislike it, and for good reasons.\nGuerilla tests are very superficial—you don’t have the time to talk to a person long enough. You can forget about a remotely representative sample, and the target audience rarely walks around the office.\nThe sample size is small due to format limitations, and you don’t have the opportunity to ask about anything even moderately complex.\nStill, some people continue to use it. Why? Because it’s quick, simple, and gives you the illusion of being informed. It feels like you’ve talked to people, listened to opinions—what’s not to like?\nWe can’t completely avoid guerilla testing, but as researchers, we can provide our colleagues with advice on making it at least somewhat useful. I’ve prepared 12 cards with tips to help smooth out most of the downsides of guerilla tests.\nSection 1: Goals and Objectives of Guerilla Testing First, let’s clarify what value guerilla tests can actually provide. We won’t learn more about the target audience. We can’t extract opinions, user problems, or feature requests from them. However, we can test the application’s interface to see if there are any obvious design flaws.\nEven if the person isn’t part of our target audience, their misunderstanding of the interface can signal to us: “Aha, something’s wrong here; we should fix it.” The exception is professional products that require specialized knowledge or education to use. Guerilla testing won’t work for a nuclear power plant interface. But for a content consumption app? Absolutely.\nText is a special case. On one hand, we can test the perception of phrases and terms. On the other hand, we must be extra careful to avoid specialized text. If our goal is to make the text understandable for everyone, it’s worth trying. If not, it’s better to skip guerilla testing.\nSection 2: Selecting Respondents Our sample will inevitably be ad hoc, but there are still some things to consider.\nFirst, obviously, don’t test on anyone who has any connection to the product being tested. Our goal is to find people who have never seen it, never participated in discussions, and ideally don’t even know it exists. We need fresh eyes, so we can skip our own team.\nRespondent rotation remains important. Avoid testing the same person twice, even with different prototypes. Playing favorites with respondents doesn’t lead to anything good, regardless of the study type.\nIdeally, we’d test with people entirely unrelated to the tech industry. Cleaners, security staff, and other support personnel are perfect for us. At the other end of the spectrum are designers and management—they see interfaces daily and are likely too experienced to provide a fresh perspective.\nPersonally, I’d be a bad respondent—over the years, I’ve encountered products so poorly designed that I’ve developed a skill for understanding even the most baffling interface solutions :)\nSection 3: Formulating Tasks Guerilla testing can easily be mistaken for quantitative research—it’s tempting to count responses and conclude one interface is better than another. In reality, it doesn’t work that way. We can’t compare test results like this. While it may look like a quantitative survey, the insights are purely qualitative—potential usability issues to watch for. So let’s not compare prototypes head-to-head; it’s not great practice even in full-scale usability tests.\nWe don’t have time to immerse participants in the testing context. We can’t spend time explaining our goals or using icebreakers. Therefore, tasks must be as short and clear as possible—like remote usability testing, but even more concise and straightforward.\nStandard research question requirements still apply: avoid leading questions, steer clear of “yes/no” questions, maintain neutrality, and account for socially desirable answers—few people want to upset a colleague.\nSection 4: Testing Guerilla testing has strict time limits. We can’t take up too much of someone’s workday. Therefore, it’s best to limit each participant to 1–2 tasks.\nAs mentioned, numbers don’t matter in guerilla testing. As they say, you don’t need to wait for a hundred cars to drive over a pothole to know it needs fixing. If we notice repeated incorrect responses, two things are clear:\n There are people who find something confusing. This isn’t a unique case; there are definitely more of them.  These are red flags and additional arguments for improving the solution.\nOh, and yes, don’t show desktop applications on a mobile device. You could run around with printouts, but the ideal approach is to display the interface on the device it’s meant for.\nConclusion Thank you for reading to the end—I hope you found this helpful. Modern product development methods push us toward fast and cheap approaches. But speed and cost-efficiency come with drawbacks: we gather less information and draw fewer conclusions. Those conclusions are less reliable. The most important thing, in my opinion, is to clearly understand the boundaries of what we’re doing. Otherwise, we risk making significant mistakes.\nYou can download the PDF with cards on my GitHub.\n","date":1631836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631836800,"objectID":"46f125f7a42244d2989273ecbce9b112","permalink":"/en/post/guerilla-testing/","publishdate":"2021-09-17T00:00:00Z","relpermalink":"/en/post/guerilla-testing/","section":"post","summary":"Guerilla testing done right","tags":["Usability Testing","Guerilla Testing"],"title":"Guerilla Testing — How Not to Shoot Yourself in the Foot","type":"post"},{"authors":null,"categories":null,"content":"I often hear the phrase \u0026ldquo;I was testing hypotheses\u0026rdquo; during discussions of research processes. Our backlog consists of hypotheses, and research reports often begin with \u0026ldquo;X out of Y hypotheses were confirmed.\u0026rdquo; Everyone talks about hypotheses: refining, approving, selecting, or revisiting them. Research activities revolve entirely around hypotheses. Hypotheses dominate the thoughts of both managers and researchers. Even the proverbial toaster tells us, \u0026ldquo;A product is a hypothesis, and 9 out of 10 products fail.\u0026rdquo;\nThe concept of a hypothesis has become so commonplace that no one questions, \u0026ldquo;What is a hypothesis?\u0026rdquo; You might think, \u0026ldquo;Isn’t it obvious? A hypothesis is a proposition.\u0026rdquo; But the obvious often turns out to be the most difficult to fully understand. Let’s dive into this sea of trivialities and try to answer the question, \u0026ldquo;What exactly is a hypothesis?\u0026rdquo;\nSo, What Is a Hypothesis? At first glance, it seems crystal clear: \u0026ldquo;A hypothesis is a proposition.\u0026rdquo; But for it to function correctly, some clarifications are required.\nFirst, a hypothesis is a proposition that can be falsified or confirmed. This is crucial. There must be no third option. Boolean logic is an essential characteristic of a hypothesis. The goal of any research is to obtain new information. Statements like \u0026ldquo;The hypothesis is neither confirmed nor refuted\u0026rdquo; are useless. If we can neither confirm nor refute a hypothesis, we’ve likely wasted time testing it.\n\u0026ldquo;But what about partially confirmed or partially refuted hypotheses?\u0026rdquo; you might argue. Unfortunately, this is not entirely accurate. Most often, partial confirmation points to either an additional factor or an incorrect conclusion. For instance: \u0026ldquo;Office workers experience Problem X.\u0026rdquo; During interviews, only office managers reported this issue. Is the hypothesis confirmed? A meticulous researcher might say, \u0026ldquo;The hypothesis is partially confirmed,\u0026rdquo; and they’d have a point.\nThis conclusion adds nuance and extra information. However, I often encounter statements like \u0026ldquo;Less than half of respondents experienced this problem, so the hypothesis is partially confirmed\u0026rdquo; or \u0026ldquo;None of the respondents experienced this problem, so we can neither confirm nor refute the hypothesis.\u0026rdquo; In the first case, the hypothesis is indeed confirmed: the problem exists among office workers. We cannot infer its prevalence based on the interviews, but the existence of the problem is proven. In the second case, we are being overly optimistic. Yes, it’s possible that none of the respondents encountered this problem or that we didn’t ask the right questions. But from the product’s perspective, it’s better to classify the hypothesis as refuted and focus on finding more widespread problems. Otherwise, we risk falling into an eternal loop of Russell’s Teapot.\nThe second important aspect is that a hypothesis must be testable. Every hypothesis must be operationalized. Hypotheses that cannot be tested are useless. This brings us to the complexity of hypotheses. A hypothesis that cannot be tested is often simply too complex to test. And complexity means expense. For example, \u0026ldquo;The product addresses Problem X\u0026rdquo; can be tested by having respondents use the product to solve the problem. However, \u0026ldquo;The product effectively addresses Problem X\u0026rdquo; is far more complicated. Defining effectiveness, finding a way to measure it, and comparing those metrics with competing products rapidly inflates the research budget. In a world of cheap and quick studies, this spells certain doom. When formulating a hypothesis, we must consider the cost and feasibility of testing it. Otherwise, we waste time and gain no new information.\nAnother pitfall is the alignment of hypotheses with research methods. Each research method implies its own set of assumptions. For instance, surveys assume respondents can form opinions about the question. Usability testing assumes respondents may make mistakes or fail tasks. Such implicit assumptions, often taken for granted, increase the complexity of hypotheses. Neglecting this can lead to several useless studies. For example, conducting a survey on which font—serif or sans-serif—is better for a website.\nA novice researcher might default to the simplest approach: \u0026ldquo;Let the users decide.\u0026rdquo; But users may not know or care about the difference between serif and sans-serif fonts. Users aren’t stupid; it’s simply not important to them. User reflection has its limits, which we should respect. Attempting to cram the uncrammable into research risks combining the incompatible.\nFor example, trying to assess the likelihood of product purchase during a usability test. Surveys are much better suited for this, and pilot sales are even more effective. Usability tests aren’t ideal here. Not only is the sample qualitative, but the respondent is already \u0026ldquo;spoiled\u0026rdquo; by knowledge of the product. However, adding one extra hypothesis can’t hurt, right? In the end, we risk falling into the illusion of awareness.\nIf you’re still reading, I assume you’ve realized by now that hypotheses are delicate instruments. A novice researcher or product manager could easily shoot themselves in the foot without noticing. And we’re not even done.\nThe Transformation of Hypotheses’ Meaning When working with stakeholders, we often ask them to propose their own hypotheses. This transforms the hypothesis from an operationalization tool to a communication tool. We start conversations with hypotheses, using them as briefs. \u0026ldquo;What’s wrong with that?\u0026rdquo; you ask. It increases flexibility, reduces redundant goal-setting steps, and accelerates the process. Plus, hypotheses add granularity: you can gather them in a basket and test them one by one.\nHowever, this approach introduces several pitfalls. Hypotheses require constant moderation, and stakeholders need ongoing education. And that’s just the tip of the iceberg. Using hypotheses as a communication tool layers additional meaning onto them. Hypotheses can now be taken personally, express doubt, or even praise or criticize an idea.\nFor example, usability test hypotheses often focus on identifying usability issues. This is implicit in the study’s purpose. As such, hypotheses should target problems. Crucially, proving the absence of problems is exceedingly difficult. The optimal approach involves hypotheses like \u0026ldquo;The respondent won’t find Button X.\u0026rdquo; By formulating such hypotheses, we pre-identify weak points, as testing every interface element would be prohibitively expensive. However, presenting these hypotheses can unintentionally communicate negative feedback to designers, which some may take personally.\nThe modern startup culture’s mantra, \u0026ldquo;A product is a hypothesis,\u0026rdquo; further distorts the concept. Essentially, it means we shouldn’t overly trust a product’s success. Yet people often interpret this metaphor literally, resulting in a Frankenstein’s monster called the \u0026ldquo;product hypothesis.\u0026rdquo; Such hypotheses often retain none of the defining characteristics of a true hypothesis.\nLet’s examine the process of testing such a hypothesis. The phrase \u0026ldquo;Every product is a hypothesis\u0026rdquo; encapsulates five layers:\n We assume there’s a need. We assume this need is widespread. We assume the product addresses this need. We assume the target audience will learn about the product. Finally, we hypothesize, \u0026ldquo;People will buy the product in sufficient quantities to cover our costs.\u0026rdquo;  However, we usually simplify the process and focus solely on the last step. Simplification is beneficial, but it sacrifices an understanding of the necessary research depth for successful product launches.\nOn the one hand, this aligns with the philosophy: \u0026ldquo;Create as many MVPs as possible, and one will succeed.\u0026rdquo; But as practice shows, the need for research doesn’t disappear. We all hover somewhere in the middle, striving to conduct research as if we’re not conducting research. This paradox leads to a curious consequence: the complexity of testable hypotheses keeps decreasing.\nSo, Do We Need Hypotheses? We need hypotheses as a research lens. They focus our efforts on specific aspects. A hypothesis is a tool for operationalization, helping us move from abstract concepts to concrete metrics, actions, and respondent statements. It’s indispensable but should remain a researcher’s tool. Misusing hypotheses as communication tools or metaphors diminishes their value as instruments. A skilled researcher knows when hypotheses are unnecessary—such as in exploratory studies or theory development. Modern mixed-method strategies often involve studies whose sole purpose is hypothesis generation. When hypotheses become briefs, we risk falling into a vicious cycle: generating hypotheses requires having hypotheses.\nWhat Should We Do Instead? In my view, we often overlook the power of research questions. A good example of a research question is: \u0026ldquo;What challenges do accountants face?\u0026rdquo; Research questions are often overshadowed by goals and tasks, relegated to the background. But a well-formulated question is often enough to draft a preliminary research plan or even a script\u0026hellip;\n","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"aa3f36a18bf051e9b88b77c522fef232","permalink":"/en/post/hypothesis/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/en/post/hypothesis/","section":"post","summary":"TL;DR: don’t test hypotheses — ask questions","tags":["Hypotheses","Democratization of research"],"title":"Everything Is a Hypothesis If You Are Brave Enough","type":"post"},{"authors":null,"categories":null,"content":"I often come across the need to count certain indexes in my work. For some of the indexes, you can find calculators (excel files where you can enter the source data and quickly get the desired index). For some of them, there are not.\nAnd it is hard to keep everything under control. You have to dig through your files every time, remember in which excel you made calculations, remember where you left the right calculator last time.\nTo put this matter in order a little and make life easier for those colleagues who do not want to re-enter the necessary formulas into Excel every time, I decided to collect calculators for those indexes that I often calculate in one excel file.\nI present to your attention the Swiss Army Research Excelbook. It contains methods for calculating most common indexes. Ideally, it should be as versatile as a Swiss army knife, so in the future I will supplement the methods and refine available calculators.\nFeatures Easy calsulation of metrics Just past raw data (answers, or answer codes), and Swiss Excelbook will calculate metrics\nTons of different metrics What calculators are already available\n  SUM (Single Usability Metric) - a single metric for measuring usability\n  UMUX (Usability Metric for User Experience) - a questionnaire for measuring the usability of a product\n  CES (Customer Effort Score) - assessment of the complexity of a particular action\n  NPS (Net Promoter Score) - measures the loyalty of customers to a company\n  CSI (Customer Satisfaction Score) - the index of satisfaction with the product\n  ODI (Outcome Driven Innovation) - determining the significance of the job statement within the jtbd and the outcome driven innovation approach (Ulvik approach)\n  Kano model - proiritize features on a product roadmap\n  PSM (Price Sensitivity Meter) - four questions to identify the optimal product price\n  Plots for every metric Every calculated metric has a plot for study presentation\nBenchmarks For some metrics there are benchmarks for ease of analysis\nHow does it work?  Download file Select the appropriate tab Insert the cleaned raw data there  The file itself has detailed instructions on what you can and can\u0026rsquo;t touch, how the raw data should look like, and some tips on using the indexes themselves\nYou can download my Swiss Excelbook here!\n","date":1615248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615248000,"objectID":"9db291a8d95487e8bb710b05fe737bab","permalink":"/en/project/swiss-excelbook/","publishdate":"2021-03-09T00:00:00Z","relpermalink":"/en/project/swiss-excelbook/","section":"project","summary":"One excelbook to rule them all!","tags":["SUM","Usability Analytics","UMUX-Lite","Kano","NPS","CSI"],"title":"Swiss army research excelbook","type":"project"},{"authors":null,"categories":null,"content":"Recently, I came across an interesting 2016 article that discusses interview questions for UX researchers applying to FAANG companies. Some of these questions really resonated with me.\nFor example:\n Imagine that you recommended fixing a usability issue, but a developer responded, “The metrics data from millions of users tells us that this issue doesn’t exist.” How would you respond?\n This is a fantastic question. It illustrates a situation that any researcher might encounter, and it also has a hidden layer, a twist. It perfectly highlights a critical issue in our industry and an important contradiction.\nIndeed, how can analytics data prove the absence of a usability issue? Especially when we observe participants encountering it during tests. What kind of data could refute this? A lack of user drop-offs on this screen? Perhaps the speed at which users navigate through this screen doesn’t slow down compared to other pages? Maybe there were pre-redesign metrics for comparison.\nTo what extent should a researcher be confident in their insights and recommendations? Can they explain to the developer the difference between proving presence and proving absence? Must a usability issue always be visible in metrics? How would the mechanism of triangulating qualitative and quantitative data work to refute it? The more you ponder the answer, the more questions arise. Honestly, I’m thrilled by this.\nI believe the genre of interview questions is a goldmine. It’s by resolving such scenarios that we grow as researchers and as an industry. Asking such questions prompts us to reflect, turning a research lens onto researchers themselves.\nI decided to try writing a few more questions with twists. Let’s try answering them together:\n  Arkady wants to conduct a quantitative survey of app users to measure UMUX. The product owner suggested recruiting an equal number of experienced and inexperienced users to enhance representativeness. How valid is this suggestion?\n  Irina is struggling to choose a research method. On the one hand, she wants to conduct in-depth interviews. But the product manager insists on running a custdev. How should she navigate this tricky situation?\n  Over the course of a week, the team gathered hypotheses for Nikolay to validate. Most of them were added to the research backlog, but one keeps bothering him: “The app will allow sorting news by the number of likes from readers.” How would you test this hypothesis if you were Nikolay?\n  During a usability test of an image-viewing app, Prokofy suddenly realized that some participants didn’t understand how the system worked. For the next test, he invited a developer to explain the framework’s intricacies to users. What would you have done in Prokofy’s place?\n  Gennady has been writing a report on five problem-solving interviews for two weeks. He’s gathered a massive amount of material, and the presentation already has 150 slides. He’s currently on the fifth slide about why participant #3 will never use the product—after all, the participant clearly stated they didn’t like the app. How can Gennady finish the report faster?\n  Mikhail gave a brilliant presentation on the usability test findings. The issues of critical severity were glaring, and the designer cried after watching the user recordings. However, it turned out that the feature being studied had already been removed from the product. This happened around the time Mikhail started conducting the first interview. What can Mikhail do next time to avoid a similar situation?\n  How would you answer these questions? What question would you ask to highlight an issue that matters most to you in the UX research industry?\n","date":1614902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614902400,"objectID":"3f3a285475af42bd3f89b954e316358e","permalink":"/en/post/hiring-researchers/","publishdate":"2021-03-05T00:00:00Z","relpermalink":"/en/post/hiring-researchers/","section":"post","summary":"To avoid asking 'Where do you see yourself in 5 years?'","tags":["Interview","Researchers"],"title":"Questions for a Researcher — In Interviews and at Work","type":"post"}]